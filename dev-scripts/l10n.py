#!/usr/bin/env python3

import argparse
import sys
import io
import contextlib
import re
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from babel.messages.frontend import CommandLineInterface
from pyaspeller import YandexSpeller

# ---------- –ü—É—Ç–∏ ----------
GUIDE_DIR    = Path(__file__).parent.parent / "documentation" / "localization_guide"
README_EN    = GUIDE_DIR / "README.md"
README_RU    = GUIDE_DIR / "README.ru.md"
LOCALES_PATH = Path(__file__).parent.parent / "portprotonqt" / "locales"
THEMES_PATH  = Path(__file__).parent.parent / "portprotonqt" / "themes"
README_FILES = [README_EN, README_RU]
POT_FILE     = LOCALES_PATH / "messages.pot"

# ---------- –í–µ—Ä—Å–∏—è –ø—Ä–æ–µ–∫—Ç–∞ ----------
def _get_version() -> str:
    return "0.1.1"

# ---------- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ README ----------
def _update_coverage(lines: list[str]) -> None:
    # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ –≤—ã–≤–æ–¥–∞ pybabel --statistics
    locales_stats = [line for line in lines if line.endswith(".po")]
    # –ò–∑–≤–ª–µ–∫–∞–µ–º (count, pct, locale) –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    rows = sorted(
        (m := re.search(
            r"""(\d+\ of\ \d+).*         # message counts
            \((\d+\%)\).*                # message percentage
            locales\/(.*)\/LC_MESSAGES  # locale name""",
            stat, re.VERBOSE
        )) and m.groups()
        for stat in locales_stats
    )

    for md_file in README_FILES:
        if not md_file.exists():
            continue

        text = md_file.read_text(encoding="utf-8")
        is_ru = (md_file == README_RU)

        # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞
        status_header = (
            "Current translation status:" if not is_ru
            else "–¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –ø–µ—Ä–µ–≤–æ–¥–∞:"
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º —à–∞–ø–∫—É –∏ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        if is_ru:
            table_header = (
                "<!-- –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏! -->\n\n"
                "| –õ–æ–∫–∞–ª—å | –ü—Ä–æ–≥—Ä–µ—Å—Å | –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ |\n"
                "| :----- | -------: | ---------: |\n"
            )
            fmt = lambda count, pct, loc: f"| [{loc}](./{loc}/LC_MESSAGES/messages.po) | {pct} | {count.replace(' of ', ' –∏–∑ ')} |"
        else:
            table_header = (
                "<!-- Auto-generated coverage table -->\n\n"
                "| Locale | Progress | Translated |\n"
                "| :----- | -------: | ---------: |\n"
            )
            fmt = lambda count, pct, loc: f"| [{loc}](./{loc}/LC_MESSAGES/messages.po) | {pct} | {count} |"

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º '---' –≤ –∫–æ–Ω—Ü–µ
        coverage_table = (
            table_header
            + "\n".join(fmt(c, p, l) for c, p, l in rows)
            + "\n\n---"
        )

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        old_block = (
            r"<!--\s*(?:–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!|Auto-generated coverage table)\s*-->"
            r".*?(?=\n(?:##|\Z))"
        )
        cleaned = re.sub(old_block, "", text, flags=re.DOTALL)

        # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
        insert_pattern = rf"(^.*{re.escape(status_header)}.*$)"
        new_text = re.sub(
            insert_pattern,
            lambda m: m.group(1) + "\n\n" + coverage_table,
            cleaned,
            count=1,
            flags=re.MULTILINE
        )

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª, –µ—Å–ª–∏ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        if new_text != text:
            md_file.write_text(new_text, encoding="utf-8")

# ---------- PyBabel –∫–æ–º–∞–Ω–¥—ã ----------
def compile_locales() -> None:
    CommandLineInterface().run([
        "pybabel", "compile", "--use-fuzzy", "--directory",
        f"{LOCALES_PATH.resolve()}", "--statistics"
    ])

def extract_strings() -> None:
    input_dir = (Path(__file__).parent.parent / "portprotonqt").resolve()
    CommandLineInterface().run([
        "pybabel", "extract", "--project=PortProtonQT",
        f"--version={_get_version()}", "--width=79", "--strip-comment-tag",
        "--no-location", f"--input-dir={input_dir}",
        f"--ignore-dirs={THEMES_PATH}",
        f"--output-file={POT_FILE.resolve()}"
    ])

def update_locales() -> None:
    CommandLineInterface().run([
        "pybabel", "update",
        f"--input-file={POT_FILE.resolve()}",
        f"--output-dir={LOCALES_PATH.resolve()}",
        "--width=79", "--ignore-obsolete"
    ])

def create_new(locales: list[str]) -> None:
    if not POT_FILE.exists():
        extract_strings()
    for locale in locales:
        CommandLineInterface().run([
            "pybabel", "init",
            f"--input-file={POT_FILE.resolve()}",
            f"--output-dir={LOCALES_PATH.resolve()}",
            f"--locale={locale}"
        ])

# ---------- –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è spellcheck ----------
IGNORED_PREFIXES = ()

def load_ignored_prefixes(ignore_file=".spellignore"):
    path = Path(__file__).parent / ignore_file
    try:
        return tuple(path.read_text(encoding='utf-8').splitlines())
    except FileNotFoundError:
        return ()

IGNORED_PREFIXES = load_ignored_prefixes() + ("PortProton", "flatpak")

# ---------- –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏ —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º ----------
speller = YandexSpeller()
MSGID_RE = re.compile(r'^msgid\s+"(.*)"')
MSGSTR_RE = re.compile(r'^msgstr\s+"(.*)"')

def extract_po_strings(filepath: Path) -> list[str]:
    # Collect all strings, then filter by ignore list
    texts, current_key, buffer = [], None, ""
    def flush():
        nonlocal buffer
        if buffer.strip():
            texts.append(buffer)
        buffer = ""
    for line in filepath.read_text(encoding='utf-8').splitlines():
        stripped = line.strip()
        if stripped.startswith("msgid ") and filepath.suffix == '.pot':
            flush(); current_key = 'msgid'; buffer = MSGID_RE.match(stripped).group(1) or ''
        elif stripped.startswith("msgstr "):
            flush(); current_key = 'msgstr'; buffer = MSGSTR_RE.match(stripped).group(1) or ''
        elif stripped.startswith('"') and stripped.endswith('"') and current_key:
            buffer += stripped[1:-1]
        else:
            flush(); current_key = None
    flush()
    # Final filter: remove ignored and multi-line
    return [
        t for t in texts
        if t.strip() and all(pref not in t for pref in IGNORED_PREFIXES) and "\n" not in t
    ]

def _check_text(text: str) -> tuple[str, list[dict]]:
    result = speller.spell(text)
    errors = [r for r in result if r.get('word') and r.get('s')]
    return text, errors

def check_file(filepath: Path, issues_summary: dict) -> bool:
    print(f"Checking file: {filepath}")
    texts = extract_po_strings(filepath)
    has_errors = False
    printed_err = False
    with ThreadPoolExecutor(max_workers=8) as pool:
        for text, errors in pool.map(_check_text, texts):
            print(f'  In string: "{text}"')
            if errors:
                if not printed_err:
                    print(f"‚ùå Errors in file: {filepath}")
                    printed_err = True
                has_errors = True
                for err in errors:
                    print(f"    - typo: {err['word']}, suggestions: {', '.join(err['s'])}")
                issues_summary[filepath].extend([(text, err) for err in errors])
    return has_errors

# ---------- –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ ----------
def main(args) -> int:
    if args.update_all:
        extract_strings(); update_locales()
    if args.create_new:
        create_new(args.create_new)
    if args.spellcheck:
        files = list(LOCALES_PATH.glob("**/*.po")) + [POT_FILE]
        seen = set(); has_err = False
        issues_summary = defaultdict(list)
        for f in files:
            if not f.exists() or f in seen: continue
            seen.add(f)
            if check_file(f, issues_summary):
                has_err = True
            else:
                print(f"‚úÖ {f} ‚Äî no errors found.")
        if has_err:
            print("\nüìã Summary of Spelling Errors:")
            for file, errs in issues_summary.items():
                print(f"\n‚úó {file}")
                print("-----")
                for idx, (text, err) in enumerate(errs, 1):
                    print(f"{idx}. In '{text}': typo '{err['word']}', suggestions: {', '.join(err['s'])}")
                print("-----")
        return 1 if has_err else 0
    extract_strings(); compile_locales()
    return 0

if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="l10n", description="Localization utility for PortProtonQT.")
    parser.add_argument("--create-new", nargs='+', type=str, default=False, help="Create .po for new locales")
    parser.add_argument("--update-all", action='store_true', help="Extract/update locales and update README coverage")
    parser.add_argument("--spellcheck", action='store_true', help="Run spellcheck on POT and PO files")
    args = parser.parse_args()
    if args.spellcheck:
        sys.exit(main(args))
    f = io.StringIO()
    with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):
        main(args)
    output = f.getvalue().splitlines()
    _update_coverage(output)
    sys.exit(0)
